{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /opt/conda/envs/pull_data/lib/python3.10/site-packages (4.1.1)\n",
      "Requirement already satisfied: wrds in /opt/conda/envs/pull_data/lib/python3.10/site-packages (3.1.6)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from wrds) (2.9.6)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from wrds) (1.24.2)\n",
      "Requirement already satisfied: sqlalchemy<2 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from wrds) (1.4.47)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from wrds) (2.0.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from wrds) (1.10.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from sqlalchemy<2->wrds) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->wrds) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->wrds) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->wrds) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->wrds) (1.16.0)\n",
      "Requirement already satisfied: pyportfolioopt in /opt/conda/envs/pull_data/lib/python3.10/site-packages (1.5.4)\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from pyportfolioopt) (2.0.1)\n",
      "Requirement already satisfied: cvxpy<2.0.0,>=1.1.10 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from pyportfolioopt) (1.3.1)\n",
      "Requirement already satisfied: scipy<2.0,>=1.3 in /home/codespace/.local/lib/python3.10/site-packages (from pyportfolioopt) (1.10.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /home/codespace/.local/lib/python3.10/site-packages (from pyportfolioopt) (1.24.2)\n",
      "Requirement already satisfied: ecos>=2 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.10->pyportfolioopt) (2.0.12)\n",
      "Requirement already satisfied: setuptools>65.5.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.10->pyportfolioopt) (66.0.0)\n",
      "Requirement already satisfied: scs>=1.1.6 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.10->pyportfolioopt) (3.2.3)\n",
      "Requirement already satisfied: osqp>=0.4.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.10->pyportfolioopt) (0.6.2.post9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2.8.2)\n",
      "Requirement already satisfied: qdldl in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from osqp>=0.4.1->cvxpy<2.0.0,>=1.1.10->pyportfolioopt) (0.1.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyportfolioopt) (1.16.0)\n",
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-t0rqsjo1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-t0rqsjo1\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 8a75a4bbb28f86f88ee2d4bd9a8c19cce444badb\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2\n",
      "  Cloning https://github.com/quantopian/pyfolio.git to /tmp/pip-install-yo9kd_d8/pyfolio_2f062abf256f43bb88bb83113bce1833\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/quantopian/pyfolio.git /tmp/pip-install-yo9kd_d8/pyfolio_2f062abf256f43bb88bb83113bce1833\n",
      "  Resolved https://github.com/quantopian/pyfolio.git to commit 4b901f6d73aa02ceb6d04b7d83502e5c6f2e81aa\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-yo9kd_d8/elegantrl_1d1ce857e7a44fea8c3a64061fb21468\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-yo9kd_d8/elegantrl_1d1ce857e7a44fea8c3a64061fb21468\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit b974a806e6235f59055c954418e54640fa549331\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wrds>=3.1.6 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (3.1.6)\n",
      "Requirement already satisfied: scikit-learn>=0.21.0 in /home/codespace/.local/lib/python3.10/site-packages (from finrl==0.3.5) (1.2.2)\n",
      "Requirement already satisfied: jqdatasdk in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (1.8.11)\n",
      "Requirement already satisfied: ray[default,tune]>=2.0.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (2.4.0)\n",
      "Requirement already satisfied: gputil in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (1.4.0)\n",
      "Requirement already satisfied: lz4 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (4.3.2)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.10/site-packages (from finrl==0.3.5) (3.7.1)\n",
      "Requirement already satisfied: yfinance in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (0.2.18)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (2.0.1)\n",
      "Requirement already satisfied: alpaca_trade_api>=2.1.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (3.0.0)\n",
      "Requirement already satisfied: exchange_calendars==3.6.3 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (3.6.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/codespace/.local/lib/python3.10/site-packages (from finrl==0.3.5) (1.24.2)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (2.6)\n",
      "Requirement already satisfied: importlib-metadata==4.13.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (4.13.0)\n",
      "Requirement already satisfied: ccxt>=1.66.32 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (3.0.82)\n",
      "Requirement already satisfied: stable-baselines3<2.0.0,>=1.6.2 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (1.8.0)\n",
      "Requirement already satisfied: gym>=0.17 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (0.21.0)\n",
      "Requirement already satisfied: stockstats>=0.4.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from finrl==0.3.5) (0.5.2)\n",
      "Requirement already satisfied: korean-lunar-calendar in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from exchange_calendars==3.6.3->finrl==0.3.5) (0.3.1)\n",
      "Requirement already satisfied: python-dateutil in /home/codespace/.local/lib/python3.10/site-packages (from exchange_calendars==3.6.3->finrl==0.3.5) (2.8.2)\n",
      "Requirement already satisfied: toolz in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from exchange_calendars==3.6.3->finrl==0.3.5) (0.12.0)\n",
      "Requirement already satisfied: pytz in /home/codespace/.local/lib/python3.10/site-packages (from exchange_calendars==3.6.3->finrl==0.3.5) (2023.3)\n",
      "Requirement already satisfied: pyluach in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from exchange_calendars==3.6.3->finrl==0.3.5) (2.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from importlib-metadata==4.13.0->finrl==0.3.5) (3.15.0)\n",
      "Requirement already satisfied: requests<3,>2 in /home/codespace/.local/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (2.28.2)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in /home/codespace/.local/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (1.26.15)\n",
      "Requirement already satisfied: msgpack==1.0.3 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (1.0.3)\n",
      "Requirement already satisfied: websockets<11,>=9.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (10.4)\n",
      "Requirement already satisfied: deprecation==2.1.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (2.1.0)\n",
      "Requirement already satisfied: aiohttp==3.8.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (3.8.1)\n",
      "Requirement already satisfied: PyYAML==6.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (6.0)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in /home/codespace/.local/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (1.5.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from aiohttp==3.8.1->alpaca_trade_api>=2.1.0->finrl==0.3.5) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from aiohttp==3.8.1->alpaca_trade_api>=2.1.0->finrl==0.3.5) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp==3.8.1->alpaca_trade_api>=2.1.0->finrl==0.3.5) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from aiohttp==3.8.1->alpaca_trade_api>=2.1.0->finrl==0.3.5) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from aiohttp==3.8.1->alpaca_trade_api>=2.1.0->finrl==0.3.5) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from aiohttp==3.8.1->alpaca_trade_api>=2.1.0->finrl==0.3.5) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from aiohttp==3.8.1->alpaca_trade_api>=2.1.0->finrl==0.3.5) (2.1.1)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from deprecation==2.1.0->alpaca_trade_api>=2.1.0->finrl==0.3.5) (23.0)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ccxt>=1.66.32->finrl==0.3.5) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /home/codespace/.local/lib/python3.10/site-packages (from ccxt>=1.66.32->finrl==0.3.5) (2022.12.7)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ccxt>=1.66.32->finrl==0.3.5) (40.0.2)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ccxt>=1.66.32->finrl==0.3.5) (66.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from gym>=0.17->finrl==0.3.5) (2.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=1.1.5->finrl==0.3.5) (2023.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (3.20.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (3.10.7)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (8.1.3)\n",
      "Requirement already satisfied: jsonschema in /home/codespace/.local/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (4.17.3)\n",
      "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (20.21.0)\n",
      "Requirement already satisfied: grpcio<=1.51.3,>=1.42.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (1.51.3)\n",
      "Requirement already satisfied: tabulate in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (0.9.0)\n",
      "Requirement already satisfied: aiohttp-cors in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (0.7.0)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (0.3.14)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (0.16.0)\n",
      "Requirement already satisfied: smart-open in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (6.3.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (1.10.7)\n",
      "Requirement already satisfied: gpustat>=1.0.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (1.1)\n",
      "Requirement already satisfied: colorful in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (0.5.5)\n",
      "Requirement already satisfied: opencensus in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from ray[default,tune]>=2.0.0->finrl==0.3.5) (0.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.0->finrl==0.3.5) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.0->finrl==0.3.5) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.0->finrl==0.3.5) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.11 in /home/codespace/.local/lib/python3.10/site-packages (from stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (2.0.0)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from wrds>=3.1.6->finrl==0.3.5) (2.9.6)\n",
      "Requirement already satisfied: sqlalchemy<2 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from wrds>=3.1.6->finrl==0.3.5) (1.4.47)\n",
      "Requirement already satisfied: thriftpy2>=0.3.9 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from jqdatasdk->finrl==0.3.5) (0.4.16)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from jqdatasdk->finrl==0.3.5) (1.0.3)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from jqdatasdk->finrl==0.3.5) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->finrl==0.3.5) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->finrl==0.3.5) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->finrl==0.3.5) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->finrl==0.3.5) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->finrl==0.3.5) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->finrl==0.3.5) (4.39.3)\n",
      "Requirement already satisfied: ipython>=3.2.3 in /home/codespace/.local/lib/python3.10/site-packages (from pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (8.12.0)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.12.2)\n",
      "Requirement already satisfied: empyrical>=0.5.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.5.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/codespace/.local/lib/python3.10/site-packages (from yfinance->finrl==0.3.5) (4.12.0)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from yfinance->finrl==0.3.5) (1.4.4)\n",
      "Requirement already satisfied: html5lib>=1.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from yfinance->finrl==0.3.5) (1.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from yfinance->finrl==0.3.5) (2.3.7)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from yfinance->finrl==0.3.5) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from yfinance->finrl==0.3.5) (4.9.2)\n",
      "Requirement already satisfied: pycares>=4.0.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from aiodns>=1.1.1->ccxt>=1.66.32->finrl==0.3.5) (4.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance->finrl==0.3.5) (2.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/codespace/.local/lib/python3.10/site-packages (from cryptography>=2.6.1->ccxt>=1.66.32->finrl==0.3.5) (1.15.1)\n",
      "Requirement already satisfied: pandas-datareader>=0.2 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from empyrical>=0.5.0->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.10.0)\n",
      "Requirement already satisfied: blessed>=1.17.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default,tune]>=2.0.0->finrl==0.3.5) (1.20.0)\n",
      "Requirement already satisfied: psutil>=5.6.0 in /home/codespace/.local/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default,tune]>=2.0.0->finrl==0.3.5) (5.9.4)\n",
      "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default,tune]>=2.0.0->finrl==0.3.5) (11.525.112)\n",
      "Requirement already satisfied: webencodings in /home/codespace/.local/lib/python3.10/site-packages (from html5lib>=1.1->yfinance->finrl==0.3.5) (0.5.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.18.2)\n",
      "Requirement already satisfied: backcall in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (2.14.0)\n",
      "Requirement already satisfied: stack-data in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.6.2)\n",
      "Requirement already satisfied: decorator in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.1.6)\n",
      "Requirement already satisfied: traitlets>=5 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (5.9.0)\n",
      "Requirement already satisfied: pickleshare in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (3.0.38)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>2->alpaca_trade_api>=2.1.0->finrl==0.3.5) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from sqlalchemy<2->wrds>=3.1.6->finrl==0.3.5) (2.0.2)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from thriftpy2>=0.3.9->jqdatasdk->finrl==0.3.5) (3.11)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (11.7.4.91)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (11.7.91)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (4.5.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (2.14.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (11.7.99)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (3.0)\n",
      "Requirement already satisfied: wheel in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (0.40.0)\n",
      "Requirement already satisfied: lit in /home/codespace/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (16.0.0)\n",
      "Requirement already satisfied: cmake in /home/codespace/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (3.26.1)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in /home/codespace/.local/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]>=2.0.0->finrl==0.3.5) (3.2.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]>=2.0.0->finrl==0.3.5) (0.3.6)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from gym>=0.17->finrl==0.3.5) (2.3.5)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from gym>=0.17->finrl==0.3.5) (2.0.5)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema->ray[default,tune]>=2.0.0->finrl==0.3.5) (0.19.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from opencensus->ray[default,tune]>=2.0.0->finrl==0.3.5) (2.11.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from opencensus->ray[default,tune]>=2.0.0->finrl==0.3.5) (0.1.3)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/codespace/.local/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]>=2.0.0->finrl==0.3.5) (0.2.6)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt>=1.66.32->finrl==0.3.5) (2.21)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]>=2.0.0->finrl==0.3.5) (2.17.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]>=2.0.0->finrl==0.3.5) (1.59.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/codespace/.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (2.1.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /home/codespace/.local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.3.5) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]>=2.0.0->finrl==0.3.5) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]>=2.0.0->finrl==0.3.5) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]>=2.0.0->finrl==0.3.5) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/envs/pull_data/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]>=2.0.0->finrl==0.3.5) (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "## install required packages\n",
    "!pip install swig\n",
    "!pip install wrds\n",
    "!pip install pyportfolioopt\n",
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl import config_tickers\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = False\n",
    "if_using_td3 = False\n",
    "if_using_sac = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 126         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -84.7       |\n",
      "|    reward             | -0.30165023 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.69        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -17.9      |\n",
      "|    reward             | -1.6832356 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -311      |\n",
      "|    reward             | 10.485874 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 61        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0177   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -14.6     |\n",
      "|    reward             | 0.7561632 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 24.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 1.24e+03   |\n",
      "|    reward             | -14.448832 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.16e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.107      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 135        |\n",
      "|    reward             | 0.47914502 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 31.6      |\n",
      "|    reward             | -4.111158 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.43      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 126        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -83.1      |\n",
      "|    reward             | -2.6628723 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 126        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 137        |\n",
      "|    reward             | -1.1701115 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 13.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -180      |\n",
      "|    reward             | -6.116576 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 22.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -344      |\n",
      "|    reward             | 5.7207823 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 71.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.0284    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -103      |\n",
      "|    reward             | -0.092922 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.93      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 2.62      |\n",
      "|    reward             | -4.616034 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.47      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 234       |\n",
      "|    reward             | 0.8909949 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 33.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 197       |\n",
      "|    reward             | 1.0947924 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 27        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -140       |\n",
      "|    reward             | 0.41661307 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 13.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -161      |\n",
      "|    reward             | 5.9110923 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 35.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.0096   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -11       |\n",
      "|    reward             | 2.0380633 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.374     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.0131    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -8.12     |\n",
      "|    reward             | 1.1667857 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.74      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -182      |\n",
      "|    reward             | -2.005862 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 30.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 126      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 83       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -56.6    |\n",
      "|    reward             | 5.517916 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 6.23     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 126        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | -123       |\n",
      "|    reward             | -12.867984 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 15.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 126       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -3.72e+03 |\n",
      "|    reward             | 23.39303  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.1e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 126        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 51.8       |\n",
      "|    reward             | 0.81538874 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -82.3      |\n",
      "|    reward             | 0.16721152 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.24       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -44.2     |\n",
      "|    reward             | 1.5995394 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.89      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -32.2     |\n",
      "|    reward             | -1.506285 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.834     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 596       |\n",
      "|    reward             | 0.9222099 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 228       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.814    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | 597       |\n",
      "|    reward             | 4.0172524 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 222       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 11.1       |\n",
      "|    reward             | 0.90218204 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.714      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 124         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 124         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 83.1        |\n",
      "|    reward             | -0.28855932 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.71        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0.0273     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | 28         |\n",
      "|    reward             | -1.7596878 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.3        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 132        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -13.4      |\n",
      "|    reward             | 0.44705522 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.94       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -2.88     |\n",
      "|    reward             | 3.472361  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 13.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 124         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 140         |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 64.3        |\n",
      "|    reward             | -0.58778685 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.86        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 124         |\n",
      "|    iterations         | 3600        |\n",
      "|    time_elapsed       | 144         |\n",
      "|    total_timesteps    | 18000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3599        |\n",
      "|    policy_loss        | -78.6       |\n",
      "|    reward             | -0.22610292 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 5.94        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 166       |\n",
      "|    reward             | 0.0987202 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 18.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 34.7       |\n",
      "|    reward             | 0.49995163 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.61       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 156        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | -208       |\n",
      "|    reward             | 0.91929287 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 28         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 160       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -168      |\n",
      "|    reward             | 4.4747806 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 22.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -0.0266   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -29.5     |\n",
      "|    reward             | 0.2359382 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.66      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | -120      |\n",
      "|    reward             | 0.9877629 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.92      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -82.6     |\n",
      "|    reward             | 3.7779706 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 176       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 51.9      |\n",
      "|    reward             | 0.8247509 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.27      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 124         |\n",
      "|    iterations         | 4500        |\n",
      "|    time_elapsed       | 180         |\n",
      "|    total_timesteps    | 22500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4499        |\n",
      "|    policy_loss        | 262         |\n",
      "|    reward             | -0.20099436 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 47.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 184       |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 143       |\n",
      "|    reward             | 3.9415822 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 16.6      |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 124           |\n",
      "|    iterations         | 4700          |\n",
      "|    time_elapsed       | 188           |\n",
      "|    total_timesteps    | 23500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.7         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4699          |\n",
      "|    policy_loss        | -77.4         |\n",
      "|    reward             | -0.0046508266 |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 6.06          |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 192        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -132       |\n",
      "|    reward             | -1.0221027 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 196       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -76.8     |\n",
      "|    reward             | 0.4102699 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.22      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 200        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 66.5       |\n",
      "|    reward             | -0.1259192 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.62       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 204        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 113        |\n",
      "|    reward             | -1.4593838 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 11.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 124      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 208      |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -329     |\n",
      "|    reward             | 4.906419 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 70.7     |\n",
      "------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3807122.05\n",
      "total_reward: 2807122.05\n",
      "total_cost: 4593.00\n",
      "total_trades: 57329\n",
      "Sharpe: 0.729\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 212        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | -28.3      |\n",
      "|    reward             | 0.23960687 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.511      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 124         |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 216         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -12.8       |\n",
      "|    reward             | -0.52362764 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.21        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 219       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -67.9     |\n",
      "|    reward             | 1.2020121 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 9.52      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 124          |\n",
      "|    iterations         | 5600         |\n",
      "|    time_elapsed       | 224          |\n",
      "|    total_timesteps    | 28000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.9        |\n",
      "|    explained_variance | -0.0909      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5599         |\n",
      "|    policy_loss        | -66.9        |\n",
      "|    reward             | -0.038960412 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 5.04         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 227        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -33.1      |\n",
      "|    reward             | -2.0600686 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 231       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -32.9     |\n",
      "|    reward             | 1.5281423 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.95      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 125         |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 235         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 3.35        |\n",
      "|    reward             | -0.26007003 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.0304      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 239        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 43.7       |\n",
      "|    reward             | 0.13921328 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 244       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -24       |\n",
      "|    reward             | -2.633047 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.33      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 249        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | 40.7       |\n",
      "|    reward             | -1.2011743 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 253       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | -324      |\n",
      "|    reward             | 2.2118473 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 99.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 124      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 257      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 57.7     |\n",
      "|    reward             | 1.053023 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.06     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 261       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 27        |\n",
      "|    reward             | -3.494632 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 4.16      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 124         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 265         |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -55.2       |\n",
      "|    reward             | 0.094100736 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 4.75        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 269       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | -0.00291  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -620      |\n",
      "|    reward             | -5.375745 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 277       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 273       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -133      |\n",
      "|    reward             | -0.971919 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 12.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 277       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -228      |\n",
      "|    reward             | 3.8419092 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 52.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 281       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 64.9      |\n",
      "|    reward             | 0.2572815 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.87      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 285       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 15.3      |\n",
      "|    reward             | 0.7474191 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.563     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 289       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -333      |\n",
      "|    reward             | 1.4626646 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 70.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 293        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0.0154     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 141        |\n",
      "|    reward             | 0.52560693 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 15.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 297        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | -216       |\n",
      "|    reward             | -4.3721943 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 25.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 301       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 309       |\n",
      "|    reward             | -5.184586 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 58.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 305       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -117      |\n",
      "|    reward             | 1.4995704 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 8.31      |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
